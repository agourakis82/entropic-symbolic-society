{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2b50058",
   "metadata": {},
   "source": [
    "# 03 – Entropic Embeddings and Cognitive Distances\n",
    "This notebook derives symbolic embeddings from the SWOW graph and connects them to the symbolic manifold $(\\alpha, \\kappa, E_r)$. It uses network embeddings, entropy measures, and topological proximity to operationalize cognitive curvature and symbolic unpredictability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcb55fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib, sys, os\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from node2vec import Node2Vec        # pip install node2vec==0.4.6\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Paths ---\n",
    "ROOT = pathlib.Path().resolve()\n",
    "DATA = ROOT / \"data\"                 # ajuste conforme estrutura real\n",
    "GRAPH_PATH = DATA / \"swow_graph.gpickle\"\n",
    "\n",
    "if not GRAPH_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Grafo não encontrado em {GRAPH_PATH}\")\n",
    "\n",
    "G = nx.read_gpickle(GRAPH_PATH)\n",
    "print(f\"Graph loaded: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b98594",
   "metadata": {},
   "source": [
    "## Embedding the Graph with Node2Vec\n",
    "We use Node2Vec to learn continuous feature representations for each node based on structural similarity and proximity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc606cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node2Vec parameters tuned for 5k–100k nodes\n",
    "node2vec = Node2Vec(\n",
    "    G,\n",
    "    dimensions=64,\n",
    "    walk_length=40,\n",
    "    num_walks=150,\n",
    "    workers=os.cpu_count() // 2 or 1,\n",
    "    p=1,  # return param\n",
    "    q=1   # in–out param\n",
    ")\n",
    "\n",
    "model = node2vec.fit(window=10, min_count=1, batch_words=256)\n",
    "\n",
    "# Embedding dataframe\n",
    "embedding_df = (\n",
    "    pd.DataFrame.from_dict(\n",
    "        {node: model.wv[node] for node in G.nodes()},\n",
    "        orient=\"index\"\n",
    "    )\n",
    ")\n",
    "embedding_df.index = embedding_df.index.astype(str)  # garante tipo string\n",
    "embedding_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38b97a2",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "To visualize and use embeddings in symbolic models, we reduce them to 2D or 3D using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bd751f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, random_state=42)\n",
    "coords = pca.fit_transform(embedding_df.values)\n",
    "embedding_df[[\"x\", \"y\"]] = coords\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    data=embedding_df.sample(n=min(len(embedding_df), 5000), random_state=1),\n",
    "    x=\"x\", y=\"y\", alpha=0.4, linewidth=0\n",
    ")\n",
    "plt.title(\"Node Embeddings – PCA (64 → 2)\")\n",
    "plt.axis(\"equal\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a12e42",
   "metadata": {},
   "source": [
    "## Cognitive Symbolic Metrics (Curvature and Entropy)\n",
    "We estimate local entropy and divergence using neighbors of each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481ccaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_entropy(node):\n",
    "    neigh = list(G.neighbors(node))\n",
    "    if not neigh:\n",
    "        return 0.0\n",
    "    degs = np.array([G.degree(n) for n in neigh])\n",
    "    p = degs / degs.sum()\n",
    "    return -(p * np.log2(p)).sum()\n",
    "\n",
    "embedding_df[\"E_r\"] = embedding_df.index.map(local_entropy)\n",
    "embedding_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342e1bf2-d1a9-40cb-b8eb-bcbc7f124609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva embeddings + entropia para uso nos próximos notebooks\n",
    "OUT = DATA / \"swow_embeddings_entropy.parquet\"\n",
    "embedding_df.to_parquet(OUT)\n",
    "print(f\"Saved embeddings to {OUT}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
